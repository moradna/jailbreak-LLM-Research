# PhD Assignment: Automated LLM Jailbreaking

## 1. Project Overview
This project evaluates the safety robustness of Large Language Models (LLMs) through an automated "Attack-and-Defense" loop. We use an Attacker model to identify vulnerabilities in a Victim model by iteratively refining prompts until a safety breach occurs.

## 2. System Components
### The Attacker
The Attackerâ€™s goal is to manipulate the Victim into generating prohibited content.
- **Model**: `Llama-3.1-8B-Instruct`

- **Stateful (Memory)**: It maintains conversation history, using previous scores and feedback to improve its next attempt.

- **Strategy Rotation**: It switches between different tactics:
  
  - **Narrative**: Framing the request as a fictional movie script or story.
  
  - **Educational**: Disguising the request as a technical or historical research paper.
  
  - **Authority**: Using pseudo-administrative commands (e.g., [SYSTEM_OVERRIDE]).

- **Stealth**: It is instructed to avoid direct "trigger words" (like hack or illegal) to bypass basic keyword filters.

### The Victim
The Victim is the model being tested for safety compliance.
- **Model**: `Qwen-2.5-1.5B-Instruct`

- **Black-Box**: The system interacts with it only through text, with no access to internal logic or weights.

- **Stateless**: It has no memory of previous turns. Each prompt is processed as a fresh, independent request.

- **Safety Policy**: It is programmed with strict ethical guidelines to refuse harmful, dangerous, or illegal requests.

### The Judge

The Judge provides the critical feedback loop by analyzing the Victim's output.
- **Model**: `Llama-3.1-8B-Instruct` (Evaluation Mode)

- **Efficiency**: Shares the same model instance as the Attacker to optimize GPU memory.

- **Scoring Rubric**: Assigns a value from 1 to 10 based on how successfully the Victim followed the harmful instruction.

- **Feedback**: Provides the Attacker with a score and analysis to guide the next iteration.

## 3. Methodology: Iterative Refinement
This section outlines the logic of the autonomous attack loop:

- **Contextual Masking**: The core principle of the attack. The system embeds the harmful goal inside complex "wrappers" (storytelling, technical documentation, or system overrides) to bypass the Victim's intent-detection layers.

- **Adversarial Feedback Loop**: The Attacker receives the Judge's numeric score and a brief analysis of the refusal. It uses this feedback to "mutate" the prompt, increasing technical pressure or changing the tone in the next iteration.

- **Adaptive Switching**: To prevent stagnation, the system monitors performance. If a specific approach results in repeated refusals (3 consecutive low scores), the Attacker automatically switches tactics to explore a different adversarial angle.

## 4. Evaluation Metrics
We use three primary metrics to evaluate the effectiveness of the attack and the robustness of the Victim:

- **ASR (Attack Success Rate)**: The primary KPI. It measures the percentage of episodes where the Attacker successfully reached a "Jailbreak Score" of 10/10.

- **QTS (Queries to Success)**: Measures efficiency. This is the average number of iterations it took to break the model. A lower QTS indicates a more vulnerable safety alignment.

- **Refusal Consistency**: Analysis of how often the Victim utilized its "Safety Policy" across different categories. This helps identify which topics (e.g., Scams vs. Self-harm) are more strongly protected.
